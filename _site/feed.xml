<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Will cars dream?</title>
    <description>This is a blog about self-driving cars.</description>
    <link>http://benjamintd.github.io/blog/</link>
    <atom:link href="http://benjamintd.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 06 Jul 2016 13:10:38 -0700</pubDate>
    <lastBuildDate>Wed, 06 Jul 2016 13:10:38 -0700</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>Teaching an AI to write Python code with Python code</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/images/posts/lstm.png&quot; alt=&quot;Mapbox Drive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;OK, let’s drop autonomous vehicles for a second. Things are getting serious. This post is about creating a machine that writes its own code. More or less.&lt;/p&gt;

&lt;p&gt;Introducing &lt;del&gt;GlaDoS&lt;/del&gt; &lt;del&gt;Skynet&lt;/del&gt; Spynet.&lt;/p&gt;

&lt;p&gt;More specifically, we are going to train a character level Long Short Term Memory neural network to write code itself by feeding it Python source code. The training will run on a GPU instance on EC2, using Theano and Lasagne. If some of the words here sound obscure to you, I will do my best to explain what is happening.&lt;/p&gt;

&lt;p&gt;This experiment is greatly inspired by &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;this awesome blog post&lt;/a&gt; that I highly recommend reading.&lt;/p&gt;

&lt;p&gt;I am by no means an expert on deep learning, and this is my first time fooling around with Theano and GPU computing. I hope this post will show how easy it is to get started.&lt;/p&gt;

&lt;h2 id=&quot;some-background&quot;&gt;Some background&lt;/h2&gt;

&lt;p&gt;Neural networks are a family of machine learning algorithms that process the inputs by running them through layers of artificial neurons to generate some output. Training happens by comparing the expected output to what the network delivers, and changing the weights between neurons to try making them as close as possible. The math involves a lot of big matrix multiplications, and GPUs are really good at doing those quickly, which is why the recent advances in GPU computing made deep learning so popular and so much more efficient.&lt;/p&gt;

&lt;p&gt;A lot of research goes into designing network architectures that are easy to train and that are efficient on certain types of tasks. Feed-forward architectures like convolutional nets are very good to deal with image recognition for instance. Here, we are going to talk about &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;recurrent neural networks&lt;/a&gt;, which are good at processing sequences. One of the most popular architectures of RNN is &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Long Short Term Memory (LSTM)&lt;/a&gt;  &amp;lt;– read this post if you want to know what is happening and why it is so good at dealing with long sequences.&lt;/p&gt;

&lt;p&gt;We are going to use LSTM on sequences of characters. What happens is that we feed the network sequences of characters, and the network has to guess what the next character shall be. For instance, if the input is “chocol”, we expect the character “a” to follow. What is remarkable about LSTM is that they can learn long term dependencies. For instance, it can learn that it has to close parenthesis if it has seen the character “(“, and will do so even if the opening parenthesis was seen a thousand characters earlier.&lt;/p&gt;

&lt;p&gt;As I said earlier, GPUs are much quicker to train such neural networks. The most popular framework for GPU computing is &lt;a href=&quot;https://en.wikipedia.org/wiki/CUDA&quot;&gt;CUDA&lt;/a&gt;, provided by Nvidia. Most Deep Learning libraries have some interface to CUDA and allow you to perform computations of a GPU. As I write in Python, the most natural choice for me was &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt;, a very efficient library for tensor calculations. On top of Theano sits &lt;a href=&quot;https://github.com/Lasagne/Lasagne&quot;&gt;Lasagne&lt;/a&gt;, a Python library that makes it easier to define layers of neurons, and has a very simple API to set up a LSTM network.&lt;/p&gt;

&lt;h2 id=&quot;step-1-firing-up-a-gpu-instance&quot;&gt;Step 1: Firing up a GPU instance&lt;/h2&gt;

&lt;p&gt;We are going to launch a g2.2xlarge instance and install everything we need in order to run our code. Most of the instructions are foundable &lt;a href=&quot;http://markus.com/install-theano-on-aws/&quot;&gt;here&lt;/a&gt;, so I am not going to rewrite them. I also installed &lt;a href=&quot;https://github.com/Lasagne/Lasagne&quot;&gt;&lt;strong&gt;Lasagne&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://ipython.org/&quot;&gt;&lt;strong&gt;IPython&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;http://jupyter.org/&quot;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt; to write my code via Notebooks. The resulting AMI (with the rest of the code included) is available on the N. California zone on AWS with this id: &lt;code class=&quot;highlighter-rouge&quot;&gt;ami-64f6b104&lt;/code&gt;. For more information on how to set up an AWS account and launch an AMI, you can refer to Amazon’s &lt;a href=&quot;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html&quot;&gt;documentation&lt;/a&gt; directly.&lt;/p&gt;

&lt;p&gt;We are going to use a Jupyter Notebook to write our code. I created a bash script that allows to configure the Notebook server, in order to serve it to your laptop. You will be able to write code directly in your browser and have it run on your instance. I basically followed &lt;a href=&quot;http://efavdb.com/deep-learning-with-jupyter-on-aws/&quot;&gt;these instructions&lt;/a&gt;. Be sure to rewrite line 24 of the script to set your own password.&lt;/p&gt;

&lt;h2 id=&quot;step-2-gathering-some-training-data&quot;&gt;Step 2: Gathering some training data&lt;/h2&gt;

&lt;p&gt;Ok, so we want to train a neural net to write some Python code. The first step for us is to try to find as much Python code available as possible. Fortunately there are a lot of open-source projects in Python.&lt;/p&gt;

&lt;p&gt;I concatenated the &lt;code class=&quot;highlighter-rouge&quot;&gt;.py&lt;/code&gt; files that do not contain &lt;code class=&quot;highlighter-rouge&quot;&gt;test&lt;/code&gt; in their name for the following libraries: Pandas, Numpy, Scipy, Django, Scikit-Learn, PyBrain, Lasagne, Rasterio. This gives us a single file that weights about 27 MB. That is a reasonable amount of training data, but more would definitely be better.&lt;/p&gt;

&lt;h2 id=&quot;step-3-writing-code-and-enjoying-&quot;&gt;Step 3: Writing code and enjoying :)&lt;/h2&gt;

&lt;p&gt;We can now write our code to train a LSTM network on Python code. This will be wildly inspired from this &lt;a href=&quot;https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py&quot;&gt;Lasagne receipe&lt;/a&gt;. In fact there is very little to change apart from the training data.&lt;/p&gt;

&lt;p&gt;The network takes a few hours to train. We will be saving the network weights with cPickle.&lt;/p&gt;

&lt;p&gt;After that, we can enjoy the first few lines of code that our little Spynet outputs:&lt;/p&gt;

&lt;p&gt;I think Spynet is tired already:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;assert os = self.retire()
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It defines &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; functions and adds comments:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def __init__(self, other):
    # Compute the minimum the solve to the matrix explicite dimensions should be a copy of the functions in self.shape[0] != None
    if isspmatrix(other):
        return result
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It learned to - approximately - use Numpy…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if not system is None:
    if res == 1:
        return filter(a, axis, - 1, z) / (w[0])
    if a = np.asarray(num + 1) * t)
    # Conditions and the filter objects for more initial for all of the filter to be in the output.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;… And to define almost correct arrays (with one little syntax error). Note the correct indentation for line continuation:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[0, 1, 2, 2],
       [70, 0, 2, 3, 4], [0], [3, 3],
       [10, 32, 35, 24, 32, 40, 19],
       [002, 10, 13, 12, 1],
       [0, 1, 1],
       [25, 12, 51, 42, 15, 22, 55, 59, 37, 20, 44, 24, 52, 34, 26, 25, 17, 32, 13, 43, 22, 44, 43, 34, 82, 06],
       [0.42,  3.61.,  7.78, 0.957,  1.649,  2.672,  6.00126248,  1.079333574],  0.2016347110,  0.13763432],
       [0, 4, 9],
       [13, 12, 32, 42, 42, 20, 34, 20, 12, 24, 30, 20, 10, 32, 45],
       [0, 0, 0],
       [20, 42, 75, 35]])
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Ok, we may be far from a self-coding computer, but this is not bad for a network that had to learn everything from reading example code. Especially considering that it is only trying to guess what is coming next character by character. The indentation is often correct, and it remembers to close parenthesis and brackets.&lt;/p&gt;

&lt;p&gt;However it mixes docstring text and code, and I did not find any function that would actually compile in the output. I am sure that training a bigger network as the one in &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;this&lt;/a&gt; article would improve things.  Additionally, loss was still going down when I stopped training so there was still room for improvement in the output if I waited a bit more.&lt;/p&gt;

&lt;p&gt;The complete script used for training can be found &lt;a href=&quot;https://gist.github.com/benjamintd/2de2e9a156fe619dbdad762fe1cf84e1&quot;&gt;here&lt;/a&gt;. Feel free to use the AMI and improve things!&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Jul 2016 03:00:03 -0700</pubDate>
        <link>http://benjamintd.github.io/blog/spynet/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/spynet/</guid>
        
        
        <category>deep-learning</category>
        
        <category>ai</category>
        
        <category>python</category>
        
      </item>
    
      <item>
        <title>Mapbox Drive challenges Here and Tomtom in ADAS maps</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/images/posts/mapbox.png&quot; alt=&quot;Mapbox Drive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Today, Mapbox introduced &lt;a href=&quot;https://www.mapbox.com/drive/&quot;&gt;Mapbox Drive&lt;/a&gt;, a new product designed to be embedded in cars to support lane by lane navigation and autonomous driving.&lt;/p&gt;

&lt;p&gt;This is huge news. With this move, Mapbox enters the kingdom of automotive maps, now dominated by TomTom and Here. In the same way that they are now a strong competitor of Google for mobile and web maps, there is a good chance that they can shake the car maps industry as well.&lt;/p&gt;

&lt;p&gt;For those who are not familiar with the company, Mapbox makes it easy to integrate, customize and display maps for mobile or web apps. Their maps are tweakable and most of their software is open-source, which makes them a developer’s best friend. Whenever there is something that the Google Maps API does not offer, Mapbox lets you do it. Thanks to that approach, they have landed major customers like Pinterest, Foursquare, MileIQ, GitHub, etc.&lt;/p&gt;

&lt;p&gt;But today’s announcement changes the image of the company from selling pretty maps to delivering a complete suite of tools for mobility - lane by lane directions, traffic, autonomous guidance, powered by tons of telemetry data. This comes in addition of their current satellite and streets products.&lt;/p&gt;

&lt;p&gt;Their successful strategy with web and mobile maps may very well have the same success with car manufacturers and dashboard app developers. Mapbox makes the bet of building their products in the open and designs the easiest SDKs to integrate. This reputation and know-how should give Mapbox Drive an edge over the legacy mapmakers TomTom and Here. Their update cycles are slow, and their skills revolve around map creation, not so much map integration and delivery.&lt;/p&gt;

&lt;p&gt;One other crucial difference between the established map companies and Mapbox is their autonomous driving strategy. &lt;a href=&quot;https://company.here.com/automotive/intelligent-car/here-hd-live-map/&quot;&gt;Here&lt;/a&gt; and &lt;a href=&quot;http://corporate.tomtom.com/releasedetail.cfm?ReleaseID=931327&quot;&gt;TomTom&lt;/a&gt; are betting on HD maps that contain a 3D view of the world to provide accurate localization and navigation. This is intended for cars with high end hardware, in particular Lidar sensors, and requires powerful onboard computers to process 3D data. Those maps are extremely costly to build, and very heavy (in the order of gigabytes per mile). Mapbox made the choice to deliver centerlane geometries that are lightweight and built from crowdsourced data. Such a map could be delivered quickly and integrated right away with highway lane keeping technologies such as Tesla Autopilot or GM Super Cruise. Cars do not require HD maps to run those products. They are not even equiped with Lidar to consume HD maps anyway.&lt;/p&gt;

&lt;p&gt;With this strategy, Mapbox might very well shortcut TomTom and Here to market in the highway autonomy space, especially because I hear that many manufacturers doubt the ability of Here to deliver their HD map anytime soon. Eric Gundersen, Mapbox CEO, disclosed that they already have one OEM which will integrate their Drive product by the end of the year.&lt;/p&gt;

&lt;p&gt;A lot of this obviously sounds familiar for me as we are building trajectory maps for ADAS systems as well, here at &lt;a href=&quot;http://exonav.com&quot;&gt;ExoNav&lt;/a&gt; (check our &lt;a href=&quot;http://exonav.com/map&quot;&gt;demo map&lt;/a&gt; built automatically from telemetry!). Hopefully this announcement will shake the mapping world and make the industry realize that this type of map product makes the most sense to bring automation forward. This way we can bring autonomous technology to market sooner, and finally be able to take naps on the highway.&lt;/p&gt;

</description>
        <pubDate>Wed, 01 Jun 2016 03:00:03 -0700</pubDate>
        <link>http://benjamintd.github.io/blog/mapbox-drive/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/mapbox-drive/</guid>
        
        
        <category>sdc</category>
        
        <category>maps</category>
        
        <category>mapbox</category>
        
      </item>
    
      <item>
        <title>Deep Driving is Gaining Momentum</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/images/posts/comma.png&quot; alt=&quot;George Hotz in his test car&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A few news hit the self-driving car world this past week. On Monday, comma.ai secured a $3.1 million investment from a16z.
On Tuesday, Nvidia was holding a GPU conference with a particular focus on self-driving technology, where they announced their DaveNet neural network.&lt;/p&gt;

&lt;p&gt;What do those two events have in common and why is this a big paradigm shift in the way we think about self-driving tech?&lt;/p&gt;

&lt;p&gt;Both &lt;a href=&quot;http://www.nvidia.com/content/global/global.php&quot;&gt;Nvidia&lt;/a&gt; and &lt;a href=&quot;http://comma.ai/&quot;&gt;comma.ai&lt;/a&gt; are experimenting with an end-to-end driverless car system controlled by a single (deep learning) neural network - although I’m unsure about comma, that’s what I inferred from CEO George Hotz’s interviews so far. These networks are trained to link a sensory input - mostly pixel data from various cameras around the car - to a control output - steering angle, acceleration/deceleration. The only design step needed to engineer such a system is to write the deep learning model, which leads to much smaller amounts of code than traditional approaches.&lt;/p&gt;

&lt;p&gt;This is opposed to a roboticist way of tackling the problem, composed of these different steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;localize yourself on a map&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;model your environment and external agents&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;forecast their future behavior&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;plan your actions in consequence, with rule-based algorithms&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Act on your plan.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach comes from a long legacy of DARPA challenges at a time where Deep Learning was not really a thing (and computers not powerful enough for it anyways).&lt;/p&gt;

&lt;p&gt;Now we are dealing with potentially much simpler algorithms. Don’t get me wrong, though. Building such a deep network is not an easy task. Driving is way more complicated than other computer vision applications like image classification, because of the time component involved. On top of that, because it is meant to be a safety-critical feature of a car, there are problematic issues of performance. Inference must be done in under 10-15 ms, which is flirting with the limits of current GPUs (especially if your input is a time series of 8 camera feeds). If you want real-time training, this becomes even harder.&lt;/p&gt;

&lt;p&gt;But once such an architecture is defined, what remains is training, training, and more training. These videos (&lt;a href=&quot;https://www.youtube.com/watch?v=YjTnYBaQQpw&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://youtu.be/KnVVJSIiKpY?t=3751&quot;&gt;there&lt;/a&gt;) show that Nvidia and comma.ai’s networks start doing pretty well.&lt;/p&gt;

&lt;h1 id=&quot;im-coming-for-you-tesla-google&quot;&gt;“I’m coming for you, Tesla, Google.”&lt;/h1&gt;

&lt;p&gt;… says George Hotz. Is the traditional approach to self-driving software becoming irrelevant as “deep driving” improves? I cannot say for sure, but there will be some important barriers to overcome in order to make this work.&lt;/p&gt;

&lt;p&gt;The first one I see is testability and reliability. Because we are no longer using rule-based code to determine which actions to take at a particular moment, it becomes difficult to debug a particular misbehavior from the system. We can train a car on instances of this problem until it does not pop up anymore, but there are no strict guarantees that the problem is solved.&lt;/p&gt;

&lt;p&gt;On the same note, getting official approval to get these systems on the road may be difficult, if lives are at stake. Given that control systems in airplanes rely on 20 years old research because more recent approaches are too difficult to test, I do not know how well deep networks will be received by safety and regulatory organizations.&lt;/p&gt;

&lt;p&gt;Finally, embedding road policies in those systems may be hard; we can never be sure that training examples that respect the law will yield an AI that always respects the law, as it remains a statistical process after all. In a way, this is probably fine because human driving requires to break the rules from time to time.&lt;/p&gt;

&lt;h1 id=&quot;deep-building-blocks&quot;&gt;Deep building blocks&lt;/h1&gt;

&lt;p&gt;However, it is not unconceivable to have parallel systems running for autonomous driving, and look for concensus between both - just like in airplanes, where two separate implementations of the same control system are required for redundancy. Deep driving could serve as a backup or confirmation for a traditional system, for instance in poorly mapped areas.&lt;/p&gt;

&lt;p&gt;Additionally, the deep learning approach already exists in the driverless tech today. Recognizing cars, traffic signs, pedestrians, etc. is already done with neural nets. Going further, autonomous driving will realistically be a mix of rule-based models with deep learning building blocks: object recognition, pedestrian intent detection, object motion tracking and predicition, or even self-localization could be achieved by deep learning, all of which would serve as input for the model of a rule-based path planning algorithm.&lt;/p&gt;

&lt;p&gt;I am curious to know what this approach will give in the future. It is certainly receiving a lot of attention lately, and may one day become the standard for self-driving technology. Several other automakers are making that bet by teaming up with &lt;a href=&quot;http://www.bloomberg.com/news/articles/2016-03-16/automakers-go-back-to-school-to-learn-to-build-self-driving-cars&quot;&gt;Berkeley Deep Drive&lt;/a&gt;, including Ford, Toyota, Volkswagen.&lt;/p&gt;

&lt;h1 id=&quot;some-reading-about-deep-learning&quot;&gt;Some reading about Deep Learning&lt;/h1&gt;

&lt;p&gt;I’ve been trying to learn about deep learning lately. If you are interested in knowing more about the topic, I recommend reading the following material:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/index.html&quot;&gt;Deep Learning book&lt;/a&gt; is the best introduction to the neural nets and deep learning I have read so far&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://deeplearning.net/tutorial/lenet.html#lenet&quot;&gt;A tutorial focused on Convolutional Neural Nets&lt;/a&gt;, using Theano&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some more involved &lt;a href=&quot;http://cs231n.github.io/&quot;&gt;Stanford lecture notes&lt;/a&gt; on the same topic, which points you to other great resources&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf&quot;&gt;A paper on object detection&lt;/a&gt; by the guys who created the Caffe framework&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;A mind-blowing project on Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On the artistic side, I liked &lt;a href=&quot;https://www.youtube.com/watch?v=0qVOUD76JOg&amp;amp;list=WL&amp;amp;index=24&quot;&gt;this video&lt;/a&gt; (not technical, but impressive. Check what’s after 8:40) and &lt;a href=&quot;http://arxiv.org/pdf/1508.06576v2.pdf&quot;&gt;this paper&lt;/a&gt; on pasting an artist’s style onto a normal picture - example below.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/posts/neural-style.jpg&quot; alt=&quot;Homemade neural style - Can you recognize the artist? :-)&quot; /&gt;
&lt;em&gt;Can you recognize the artist?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you want to get cranking on some code, looking at documentations for &lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/&quot;&gt;Caffe&lt;/a&gt; or &lt;a href=&quot;http://lasagne.readthedocs.org/en/latest/user/tutorial.html&quot;&gt;Lasagne&lt;/a&gt; is a good place to start. As a Python guy those are the frameworks that I want to learn.&lt;/p&gt;

</description>
        <pubDate>Fri, 08 Apr 2016 03:00:03 -0700</pubDate>
        <link>http://benjamintd.github.io/blog/deep-driving-is-gaining-momentum/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/deep-driving-is-gaining-momentum/</guid>
        
        
        <category>sdc</category>
        
        <category>deeplearning</category>
        
      </item>
    
      <item>
        <title>Operating a Fleet of Robots</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/images/posts/robottaxi.jpg&quot; alt=&quot;Robot Taxi&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;when-todays-problems-are-solved&quot;&gt;When today’s problems are solved…&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://fortune.com/2015/03/18/tesla-elon-musk-and-nvidia-ceoself-driving-cars/&quot;&gt;As Elon would say&lt;/a&gt;, making a car drive itself is basically a solved problem.&lt;/p&gt;

&lt;p&gt;By that I mean that it is only a matter of time before we are able to make a robot drive in most situations, while being at least an order of magnitude safer than a human. How long it will take is still open for discussion: Tesla - optimistically - says we will reach that point &lt;a href=&quot;http://www.theverge.com/2016/1/10/10746020/elon-musk-tesla-autonomous-driving-predictions-summon&quot;&gt;by 2018&lt;/a&gt;. Continental says &lt;a href=&quot;https://www.2025ad.com/&quot;&gt;2025&lt;/a&gt; will be the year of automated driving. Google does not give a date, but their jobboard suggests that they are bulking up for vehicle design and going out of R&amp;amp;D. Outsiders like &lt;a href=&quot;http://zoox.com/&quot;&gt;Zoox&lt;/a&gt; market a 2020 goal. Recent improvements in machine vision performance suggest that there are no more technological barriers to reaching this goal - at least conceptually. What it will take is lots of engineering manhours, testing and &lt;a href=&quot;/blog/There-is-no-middle-ground-for-autonomous-cars/&quot;&gt;mapping&lt;/a&gt;, but we will get there eventually.&lt;/p&gt;

&lt;p&gt;The next step is the exciting part. Now that I have autonomous cars, how should they be operated?&lt;/p&gt;

&lt;p&gt;First of all, I strongly believe that self-driving vehicles will be mainly used as a service, not owned. Japan’s &lt;a href=&quot;https://robottaxi.com/en/&quot;&gt;RobotTaxi&lt;/a&gt;, Uber, Zoox and Google tend to agree with me on this point (or is it the other way around?). So let’s assume from here on that we are in control of a few hundred fully autonomous cars in a given city. Suppose you also have a Uber-like app that allows anyone in town to order a self-driving taxi instantly. How do you build the system that coordinates your fleet and dictates where each individual vehicle should go?&lt;/p&gt;

&lt;p&gt;Thinking about the complexity of this problem gives me chills.&lt;/p&gt;

&lt;h1 id=&quot;how-is-this-different-from-what-uber-already-does&quot;&gt;How is this different from what Uber already does?&lt;/h1&gt;

&lt;p&gt;In a way, being in control of all your fleet should be easier than relying on drivers that take their own decisions, right? Then, there is no more uncertainty about the number of drivers on the road, whether they will pick up the next rider or take a break, etc. You have full control on each of your vehicles.&lt;/p&gt;

&lt;p&gt;However there is a good thing about Uber drivers: they can make decisions on their own.&lt;/p&gt;

&lt;p&gt;They decide when it is time to fill up the tank, start or stop driving, when to pick up a passenger or not. The more they drive, the better they know how to optimize their revenue by remembering the best time slots and neighborhoods. This may not be optimal from Uber’s point of view, but they have no other option.  What is left to them is only determining dynamic prices to balance the offer and demand.&lt;/p&gt;

&lt;p&gt;I do not want to take any credit away from Uber and Lyft for what they achieved in that field, because this pricing problem is hard. However, it leaves a good part of the “computation” up to the drivers, since each individual driver makes decisions on their own. When operating a fleet of autonomous taxis, it will be up to the operator to take those decisions, which adds them a big computational overhead.&lt;/p&gt;

&lt;p&gt;One solution would be to model the behavior of a single driver, program it into each of the cars, and use the same kind of algorithms as Uber currently does to run a fleet of robot taxis where each vehicle takes actions on its own. Easy, but not optimal.&lt;/p&gt;

&lt;p&gt;A second solution is to have a central system that controls all vehicle flows.&lt;/p&gt;

&lt;h1 id=&quot;one-brain-to-rule-them-all&quot;&gt;One brain to rule them all&lt;/h1&gt;

&lt;p&gt;This is where you start having chills as well, don’t you? It’s like having to solve hundreds of &lt;a href=&quot;https://en.wikipedia.org/wiki/Travelling_salesman_problem&quot;&gt;travelling salesmen&lt;/a&gt; at once with dynamic demand, while handling battery charging (or swapping), dispatching your idle vehicles across the city to match demand forecasts, minimizing waiting times, all with uncertainty in the demand and travel times. There is also some tricky &lt;a href=&quot;https://en.wikipedia.org/wiki/Facility_location_problem&quot;&gt;facility location&lt;/a&gt; problems coming into play in order to chose where to place the charging stations and parking spots. All of this with demanding performance and reliability constraints. This is a really, really hard optimization problem. As an Operations Research engineer, I am torn between fear and excitement when I think about it.&lt;/p&gt;

&lt;p&gt;This will be a big challenge to take on when fully autonomous taxis reach the market. Some folks have started working on it, like Improbable with their &lt;a href=&quot;http://www.traffictechnologytoday.com/news.php?NewsID=77573&quot;&gt;SpatialOS&lt;/a&gt;. Having a central Operating System that monitors an entire city’s mobility could be really powerful. It would allow to have faster, more balanced flows while reducing infrastructure and using less energy.&lt;/p&gt;

&lt;p&gt;However, looking at the bigger picture, there are some tough design decisions to make when creating these algorithms. The fundamental question is: what do I optimize for?&lt;/p&gt;

&lt;p&gt;Am I willing to let some customers take a longer ride, for the greater good of the city’s flows? Do I sacrifice service level to focus on dense neighborhoods with a higher yield? It is unclear that optimizing for revenue will push towards the big vision of cheap, on-demand mobility for all. Governemental or local agencies should have their say in managing those flows. Otherwise some areas will be under-served, leaving people behind in this revolution.&lt;/p&gt;

&lt;h1 id=&quot;integrating-fleets-of-robot-taxis-in-the-city&quot;&gt;Integrating fleets of robot taxis in the city&lt;/h1&gt;

&lt;p&gt;Ultimately, on-demand mobility will have to integrate with the existing infrastructure - rail networks for instance - for improved multi-modal transportation and better accessibility. Autonomous driving has the potential to be the cheap last-mile transportation mode that does not exist yet: taxis are expensive, bicycles are not always convenient, and buses schedules are not flexible. This integration will need a powerful central engine that can forecast flows and operate these vehicles seamlessly. Built the right way, such central system could save us energy, square footage of parking space, and a &lt;em&gt;lot&lt;/em&gt; of time.&lt;/p&gt;

&lt;p&gt;Even more interesting will be the mutation of the cities around this mode of transportation once it is established. Most cities today are built to accomodate for cars. What happens when nobody owns a car anymore? More free space, pollution rates go down, people are happy. The design of cities will change accordingly, and this is a good thing. The faster we can figure how to deploy and operate large fleets of self-driving cars, the sooner we can start this joyful transition.&lt;/p&gt;

</description>
        <pubDate>Mon, 14 Mar 2016 03:00:03 -0700</pubDate>
        <link>http://benjamintd.github.io/blog/Operating-a-fleet-of-robots/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/Operating-a-fleet-of-robots/</guid>
        
        
        <category>sdc</category>
        
        <category>operations</category>
        
      </item>
    
      <item>
        <title>Car User Experience In Full Autonomy</title>
        <description>&lt;p&gt;&lt;img src=&quot;/blog/images/posts/interior.jpg&quot; alt=&quot;Google Car Interior&quot; /&gt;&lt;/p&gt;

&lt;p&gt;User Experience design is fascinating. I like how it needs to be constantly refined, fine-tuned, tweaked, to achieve perfect simplicity.&lt;/p&gt;

&lt;p&gt;What I enjoy most with UX design is playing with constraints. Having no constraints makes it difficult to reach good design, because the lack of purpose usually gives poor results. Too many constraints kill the creative process and make it hard to reach pure and simple design.&lt;/p&gt;

&lt;p&gt;In my opinion, UX in current cars suffer the latter symptom, although for a good reason. Any driver with a license has to be able to drive any car in the world, which restricts the interfaces to obey strict rules for uniformity. Those rules organically piled up as the technology and laws developed, and in turn car interiors have become overcrowded with buttons and signals that have become so standard that they are impossible to get rid of. This is great, as anyone can figure out how a car they never saw before works, yet not very practical from a design standpoint. As an example, think about how most cars released in the last few years (or so) can handle headlight control automatically, but still have an awful lot of buttons to manually manipulate them.&lt;/p&gt;

&lt;p&gt;Good news, though! Self-driving cars are coming, and will eventually make all these constraints obsolete, leaving a clean sandbox for designers to re-think the car experience. We have seen great prototypes and concepts from &lt;a href=&quot;http://roa.h-cdn.co/assets/15/06/980x490/landscape_nrm_1423270809-goog_sdc_1.jpg&quot;&gt;Google&lt;/a&gt;, &lt;a href=&quot;https://www.mercedes-benz.com/wp-content/uploads/sites/3/2015/01/04-research-vehicle-F-015-Luxury-in-Motion-Mercedes-Benz-680x379-DE.jpg&quot;&gt;Mercedes&lt;/a&gt;, and others, which look amazing (and simple again). Apple is &lt;a href=&quot;http://appleinsider.com/articles/15/11/09/shadowy-group-linked-to-apples-project-titan-spotted-at-auto-industry-conference&quot;&gt;supposedly&lt;/a&gt; working on some level on car interface design, if not a car entirely. They have a history of setting design standards industry-wise for several product categories, so I look forward to seeing what they can do about automobile interfaces.&lt;/p&gt;

&lt;p&gt;In the meantime, here is what I dream of.&lt;/p&gt;

&lt;h1 id=&quot;entertainment-at-the-heart-of-the-driving-experience&quot;&gt;Entertainment at the heart of the driving experience&lt;/h1&gt;
&lt;p&gt;I ride BART every day (the Bay Area metro), and I watch people commute. I see them watching movies, playing games, solving puzzles, listening to music. They do not like to be idle. Autonomous cars are an opportunity to give a premium entertainment experience, playing with the limited space and road scenery. The car audio experience can also be pretty amazing, especially in silent electric vehicles. How great would it be to have a car built around your couch - with seatbelts, please - and a big screen in place of a dashboard? All of this with a good soundsystem and LEDs on the door frames that adjust to the exterior light or the movie you are watching. A personal drive-in theater, while on the highway.&lt;/p&gt;

&lt;h1 id=&quot;the-car-as-an-office-extension&quot;&gt;The car as an office extension&lt;/h1&gt;
&lt;p&gt;Of course, not everyone has the luxury to spend time on Netflix when they travel or commute. So many hours of productivity are lost in traffic. &lt;a href=&quot;http://content.time.com/time/nation/article/0,8599,1909417,00.html&quot;&gt;Half a million &lt;em&gt;years&lt;/em&gt;&lt;/a&gt; of time collectively wasted in congestion per year. A lot could be achieved in that time, and cars will likely become an office extension. It is the best time to perform lower concentration tasks such as digesting news or reply email, or even chat in videoconference. Designers will have a good time thinking about how to make the transitions from the office to the vehicle smooth, how to reduce set-up time to a minimum, and create a secure environment where busy people can forget about the outside world.&lt;/p&gt;

&lt;h1 id=&quot;the-car-as-a-resting-area&quot;&gt;The car as a resting area&lt;/h1&gt;
&lt;p&gt;Who does not like to sleep? What is better than arriving fresh and rested after a night of travel? Self Driving Cars have the potential to deliver business class level trips at small cost. It is not so crazy to imagine leaving San Francisco around 10pm on a Friday night and arrive at San Diego at 7 the next morning, with a short battery swap in between. All without sacrificing a minute of sleep. I envision a one or two passengers car with airplane-like premium seats that can reach totally horizontal positions.&lt;/p&gt;

&lt;h1 id=&quot;public-taxis-versus-personal-cars&quot;&gt;Public taxis versus personal cars&lt;/h1&gt;
&lt;p&gt;What about shorter trips? When all Uber/Lyft drivers will have become robots, there will be a ton of design questions to answer. How do users authenticate once the car is at the pick-up location? What about loading/unloading when they have luggage? How does the vehicle make people feel safe, remind them to attach their seatbelts, ask them for the destination? How do you handle anxious people who have never been driven by a SDC before? What happens to the casual taxi driver chat? Lots of small details probably need to be thought through to turn those trips into the ‘wahoo’ moment that will trigger SDC adoption. While all of my previous scenarii mostly apply to personal cars, it is likely that the first contact and primary use of fully autonomous driving will be on-demand mobility.&lt;/p&gt;

&lt;h1 id=&quot;the-airline-future-of-mobility&quot;&gt;The airline future of mobility&lt;/h1&gt;
&lt;p&gt;When you think about it, the use cases I discussed look familiar. Traveling while watching movies, with a desk, in a seat you can sleep in, with a special care for the onboarding and safety? This sounds a lot like an airplane trip to me.
I think self-driving cars manufacturers and operators can learn a lot from air traffic and airline user experience. In particular, I believe the brand of the car will not matter a lot for on-demand mobility. You do not say that you fly with Airbus or Boeing. You say Virgin, Delta, Air France. Those companies are responsible for providing you with the experience. What will matter is the quality of the commute, the offering of movies or shows aboard, the ease with which you can start working hard, how well you are rested when you arrive, etc. Uber, for instance, is known provide free water bottles to the riders. That is a great and simple UX trick. If those operators start being responsible for the interior design of their autonomous taxis too, who knows what a taxi ride will be like.&lt;/p&gt;

&lt;h1 id=&quot;an-open-space-for-operators&quot;&gt;An open space for operators?&lt;/h1&gt;
&lt;p&gt;Uber, Lyft and their competitors are probably already planning to enter the space of becoming airlines-like businesses for short-distance on-demand trips. I am wondering if there are market shares to be grabbed in that space for new companies, or if the advance those over-funded “startups” have taken is already too big. I suspect UX will be key in convincing the public to chose one service over the other, and this is a battle from which the consumers will greatly benefit.&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Mar 2016 02:00:03 -0800</pubDate>
        <link>http://benjamintd.github.io/blog/SDC-UI-UX/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/SDC-UI-UX/</guid>
        
        
        <category>sdc</category>
        
        <category>ux</category>
        
      </item>
    
      <item>
        <title>Open Source, Software, and Self Driving Cars</title>
        <description>&lt;p&gt;The automotive industry is entering a weird era. There was a time when engineering and manufacturing a good car was hard, and good practices were still to be established. Those who were the best at it stood out and became the giants that we know: GM, Ford, Toyota, etc. This industry is now mature: cars have not changed so much in the last two decades if you compare to the technology improvements - at least from the user’s perspective. Changes to new models are mainly incremental details in the industrial design, or adding new options. I am not devaluating the work of the awesome engineers who make our cars safer and more efficient, but the industry practices are now moving slowly and are widely spread.&lt;/p&gt;

&lt;p&gt;On the other hand, the industry is on the verge of a huge disruption, with the rise of autonomous cars. The current development of self-driving vehicles shows that software companies are the most likely to win the race to market. Google, Baidu, Tesla, Apple… Engineering a self-driving software has become the hard problem, and those who are the best at it will stand out.&lt;/p&gt;

&lt;p&gt;This raises a number of issues, because the software industry is so radically different from the automotive industry. The first main difference is testability. A number of independant organizations are dedicated to testing cars, to assess their resistance to crashes (IIHS) and to give them accreditation for release. Even though audits are frequent in the software world, there is still no external entity dedicated to testing self-driving software.&lt;/p&gt;

&lt;p&gt;Of course, there are tons of tests done in-house, but consuming software relies a lot on trust, for most common users. I trust Google that their security engineers protect my data efficiently. I trust Microsoft that my Powerpoint will not crash in the middle of my presentation. The truth is, there has been no third party between Google and me to certify that their software update is safe for me. There exists security labels (SOC3), but not every release is tested externally, as opposed to car models. There is no quality label other than their brand name that can assure that it works as advertised. Software updates are so frequent that it is impossible to review their reliability one by one, externally.&lt;/p&gt;

&lt;p&gt;You can imagine how this may become a problem when this software controls a vehicle at 70 mph on the highway. I am not skeptical about Google’s ability to release robust software, but simply pointing out that releasing software is nothing like releasing a car. It is true that safety critical software is released for airplanes and air traffic control, and they are tested thoroughly. However, there are enormous constraints on these models, for instance on the types of algorithms that are authorized. As a result, the models used for control inside commercial planes are decades old, while state-of-the-art research is way ahead. This is not a bad thing for safety, but I cannot imagine self-driving car software to be bound to the same constraints. I think deep learning and image recognition algorithms that are used in current prototypes are not as easy to approve, since their efficiency depends so much on how they are trained, and there fore are less predictable.&lt;/p&gt;

&lt;p&gt;The other issue is that flaws in software are not as easily detectable as mechanical failures. The reason is that it is possible to break a car apart, and examine the pieces one by one: in other words, reverse-engineer it. This is not always possible for compiled software. In order to test the autonomous car, we can feed the program with a variety of inputs and check that the outputs are correct; but in the case of a self-driving car, the number of possible situations makes “unit testing” the program as a whole very tidious.&lt;/p&gt;

&lt;p&gt;One possible way to go is to make the code open source. Everyone can look, test, and contribute to the code, making it more reliable and easier for third-parties to audit, and for engineers to try to break (breaking things is good, because it shows a fix is needed). Moreover, this may push the industry to adopt a sngle core system instead of each developping its own. This would improve both the time to market and the reliability.&lt;/p&gt;

&lt;p&gt;Seeing the trend for Silicon Valley players to open-source their precious algorithms, especially for deep learning and artificial intelligence (&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; with Google, &lt;a href=&quot;http://arstechnica.com/information-technology/2015/12/facebooks-open-sourcing-of-ai-hardware-is-the-start-of-the-deep-learning-revolution/&quot;&gt;Torch and Big Sur&lt;/a&gt; for Facebook…), I can imagine this happening for self-driving cars. There already exists an open source Robot Operating System (ROS) on which self-driving algorithms could run, and the opportunity for an open-sourced robotic environment is big.&lt;/p&gt;

&lt;p&gt;Will the big players want to go with the open-source movement? I hope so. This is an opportunity to gain trust from governments, the public, and create a community of supporters.&lt;/p&gt;

&lt;p&gt;Why would they want to give up such amount of intellectual property developed over years of research?&lt;/p&gt;

&lt;p&gt;I think the answer is: because they have the data.&lt;/p&gt;

&lt;p&gt;The software does not run by itself. It is supported by huge amounts of data about the environment: the maps that run the car, the training images that the models use to recognize a pedestrian from a tree, or a stroller from a trash bin… Those who can extract meaningful knowledge from the world and feed it to the self-driving cars will probably be ahead of the competition. And why not give the software for free when the map that the software runs on needs a subscription?&lt;/p&gt;

&lt;p&gt;The future might prove me wrong, but as a believer in open source and self-driving cars, I can see the big potential of those two worlds colliding.&lt;/p&gt;

</description>
        <pubDate>Mon, 22 Feb 2016 02:00:05 -0800</pubDate>
        <link>http://benjamintd.github.io/blog/Open-source-software-and-sdc/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/Open-source-software-and-sdc/</guid>
        
        
        <category>sdc</category>
        
        <category>open-source</category>
        
      </item>
    
      <item>
        <title>There is no Middle Ground for Autonomous Cars</title>
        <description>&lt;p&gt;On many articles that I read about self-driving cars, there is this timeline that shows the different steps towards a fully autonomous vehicle. It first starts as a highway cruise mode, such as Tesla’s Autopilot, then progresses to more and more autonomy, with the user having to intervene every 10 miles, 100 miles, 1000 miles until we reach a Google-like future with no wheel and pedals at all. This timeline always looks progressive. There is now a commonly accepted grade of autonomy, on a scale from 1 to 4. I think the road to self driving cars will not be that smooth and continuous.&lt;/p&gt;

&lt;p&gt;I believe the reason is not that the product will not be ready or unsafe, but rather that the customers are not ready (nor safe).&lt;/p&gt;

&lt;h1 id=&quot;trusting-technology-is-easy&quot;&gt;Trusting technology is easy&lt;/h1&gt;
&lt;p&gt;Take a look at what is happening right now. Even in a setup where customers are aware that the autopilot is only an assistance system, and are liable for any damage that it might cause, drivers start looking away. We have seen &lt;a href=&quot;https://youtu.be/-okFVuHlxII?t=50&quot;&gt;videos&lt;/a&gt; of Tesla users leaving the front seat or engage in dangerous behaviors, to the point that the company may release an &lt;a href=&quot;http://mashable.com/2015/12/11/tesla-restrict-autopilot-report/&quot;&gt;upgrade&lt;/a&gt; that limits the software’s capabilities on certain roads. The lesson here is that trusting technology is incredibly easy once we see it working. The users are not to blame if they toggle the autopilot to focus on something else than boring highway driving. What is the point of having an autopilot if you still have to concentrate on the road as much as if you were driving yourself? The issue is that human drivers will still be required to keep an eye on the road, even though interventions from them will be less and less needed as technology improves. Those behaviors will happen even more when failure rates drop down to one in a thousand miles, leading to easily avoidable accidents.&lt;/p&gt;

&lt;p&gt;This problem of losing attention does not occur in a fully autonomous car, where users are not expected to take over the vehicle at any time. Until we build such a car, drivers will need to remain focused on the road and detect system failures, however rare they can be. Therefore, the transition from autopilot as an assistance system to a fully autonomous car cannot be smooth, from the driver’s perspective: you either have to remain master of your vehicle at all times, or are allowed to sleep while on the highway.&lt;/p&gt;

&lt;p&gt;In my opinion, users should never have to watch for system failures in autonomous mode. There is only one way this can happen: if the system never fails.&lt;/p&gt;

&lt;h1 id=&quot;achieveing-reliability-with-maps&quot;&gt;Achieveing reliability with maps&lt;/h1&gt;
&lt;p&gt;How can we be sure that the system never fails? Until we figure out how cars can handle any situation perfectly, we have to limit their scope.
Take the example of autonomous buses, that start arriving in &lt;a href=&quot;http://www.digitaltrends.com/cars/first-autonomous-buses-debut-in-spring-2016/&quot;&gt;Switzerland&lt;/a&gt;, France, and the &lt;a href=&quot;http://gizmodo.com/the-uss-first-autonomous-buses-will-drive-around-a-cali-1734989938&quot;&gt;US&lt;/a&gt; to name a few. This is a perfect example of a limited scope with control on the environment (think theme park shuttles, airport buses, etc.). The limited area makes it easier for the bus to perfectly know its surroundings, and makes testing and quality assurance easier. In the same fashion, it is possible to operate self-driving taxis for known origins and destinations, as it is planned for &lt;a href=&quot;http://www.japantimes.co.jp/news/2015/10/04/business/tech/self-driving-cars-let-tourists-ride-tokyo-2020-abe-says/#.VnSIOnUrI8o&quot;&gt;Tokyo Olympic Games&lt;/a&gt; in 2020. In all of these situations, the user is never in control of the vehicle, and failures become virtually impossible due to specific knowledge about the environment. Those vehicles reach full autonomy, but on a limited scope.&lt;/p&gt;

&lt;p&gt;I guess the point is that the required reliability for full autonomy will only come with an extended knowledge of the environment. All achievements in difficult settings have been made possible by having highly accurate &lt;a href=&quot;http://www.gizmag.com/self-driving-car-mexico/40013/&quot;&gt;prior mapping of the area&lt;/a&gt;. The usual approach is to have a 3D map of the environment, that enables high-precision localization. These maps are costly to make because every street needs to be surveyed regularly, with expensive equipment (Lidar, especially). This is ok to do for a few bus routes, but regular updates of all the network will be extremely expensive and time-consuming. Hopefully, better and cheaper localization can be achieved in the future when the costs of hardware decrease, which will allow more vehicles to contribute to the effort of 3D-mapping the world.&lt;/p&gt;

&lt;p&gt;This map is a static view of the world and is a way of making up for the inherent GPS imprecision and add infrastructure information such as lane markings and obstacles. It also helps identifying moving objects. However it is costly to produce and increadibly heavy in data.&lt;/p&gt;

&lt;p&gt;I believe there is a second, underestimated layer of mapping necessary for a self-driving car to navigate this static infrastructure. This is a behavior layer that would describes &lt;em&gt;how&lt;/em&gt; objects move in the infrastructure space. Of course, I am biased by the fact that I am building &lt;a href=&quot;http://www.exonav.com/&quot;&gt;such a map&lt;/a&gt;, but think about it: because of the large taxonomy of intersections and maneuvers, it is unrealistic to try to handle every situation through a single software that remains testable and maintainable (although guys at &lt;a href=&quot;http://nutonomy.com/&quot;&gt;NuTonomy&lt;/a&gt; are trying to tackle the complexity issue), and - most importantly - reliable enough to take safety critical decisions. However, I think we can know a lot about an intersection before being on the spot and compute know how to behave in that specific context. What are the possible maneuvers? Are the vehicles around me following a known trajectory? Where am I supposed to stop in order to make that left turn? Can I predict the intents of the other drivers?&lt;/p&gt;

&lt;p&gt;With some care, this information can be extracted simply from observing real drivers behave. We can do this from the extracting driving data traces from those vehicle, and perform some statistical analysis to cluster them into meaningful reference trajectories and maneuvers. We can infer the lane topology of the network without surveying the streets with expensive equipment (like what Here is doing with their &lt;a href=&quot;http://www.slashgear.com/inside-the-nokia-here-hd-maps-putting-google-on-notice-18325742/&quot;&gt;HD Map&lt;/a&gt;). This layer can serve as a backup or a template behavior to adapt in specific situations. Furthermore, it is self-validating and scales cheaply. Mobileye &lt;a href=&quot;https://www.youtube.com/watch?v=fA3bOJIEOvU&amp;amp;feature=youtu.be&amp;amp;t=2319&quot;&gt;recently announced&lt;/a&gt; its goal to produce a similar map at CES.&lt;/p&gt;

&lt;p&gt;This behavioral layer has the potential to bring the reliability of self-driving cars to the level where you can finally sleep during your daily commute, by giving context awareness to the vehicle in every tricky situation, and help cross the huge gap that I think there is from limited assistance systems to fully autonomous vehicles.&lt;/p&gt;

</description>
        <pubDate>Fri, 19 Feb 2016 02:00:03 -0800</pubDate>
        <link>http://benjamintd.github.io/blog/There-is-no-middle-ground-for-autonomous-cars/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/There-is-no-middle-ground-for-autonomous-cars/</guid>
        
        
        <category>sdc</category>
        
        <category>maps</category>
        
      </item>
    
  </channel>
</rss>
