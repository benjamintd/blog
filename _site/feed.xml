<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Will cars dream?</title>
    <description>This is a blog about self-driving cars.</description>
    <link>http://benjamintd.github.io/blog/</link>
    <atom:link href="http://benjamintd.github.io/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 22 Feb 2016 16:35:20 -0800</pubDate>
    <lastBuildDate>Mon, 22 Feb 2016 16:35:20 -0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Open Source, Software, and Self Driving Cars</title>
        <description>&lt;p&gt;The automotive industry is entering a weird era. There was a time when engineering and manufacturing a good car was hard, and good practices were still to be established. Those who were the best at it stood out and became the giants that we know: GM, Ford, Toyota, etc. This industry is now mature: cars have not changed so much in the last two decades if you compare to the technology improvements - at least from the user’s perspective. Changes to new models are mainly incremental details in the industrial design, or adding new options. I am not devaluating the work of the awesome engineers who make our cars safer and more efficient, but the industry practices are now moving slowly and are widely spread.&lt;/p&gt;

&lt;p&gt;On the other hand, the industry is on the verge of a huge disruption, with the rise of autonomous cars. The current development of self-driving vehicles shows that software companies are the most likely to win the race to market. Google, Baidu, Tesla, Apple… Engineering a self-driving software has become the hard problem, and those who are the best at it will stand out.&lt;/p&gt;

&lt;p&gt;This raises a number of issues, because the software industry is so radically different from the automotive industry. The first main difference is testability. A number of independant organizations are dedicated to testing cars, to assess their resistance to crashes (IIHS) and to give them accreditation for release. Even though audits are frequent in the software world, there is still no external entity dedicated to testing self-driving software.&lt;/p&gt;

&lt;p&gt;Of course, there are tons of tests done in-house, but consuming software relies a lot on trust, for most common users. I trust Google that their security engineers protect my data efficiently. I trust Microsoft that my Powerpoint will not crash in the middle of my presentation. The truth is, there has been no third party between Google and me to certify that their software update is safe for me. There exists security labels (SOC3), but not every release is tested externally, as opposed to car models. There is no quality label other than their brand name that can assure that it works as advertised. Software updates are so frequent that it is impossible to review their reliability one by one, externally.&lt;/p&gt;

&lt;p&gt;You can imagine how this may become a problem when this software controls a vehicle at 70 mph on the highway. I am not skeptical about Google’s ability to release robust software, but simply pointing out that releasing software is nothing like releasing a car. It is true that safety critical software is released for airplanes and air traffic control, and they are tested thoroughly. However, there are enormous constraints on these models, for instance on the types of algorithms that are authorized. As a result, the models used for control inside commercial planes are decades old, while state-of-the-art research is way ahead. This is not a bad thing for safety, but I cannot imagine self-driving car software to be bound to the same constraints. I think deep learning and image recognition algorithms that are used in current prototypes are not as easy to approve, since their efficiency depends so much on how they are trained, and there fore are less predictable.&lt;/p&gt;

&lt;p&gt;The other issue is that flaws in software are not as easily detectable as mechanical failures. The reason is that it is possible to break a car apart, and examine the pieces one by one: in other words, reverse-engineer it. This is not always possible for compiled software. In order to test the autonomous car, we can feed the program with a variety of inputs and check that the outputs are correct; but in the case of a self-driving car, the number of possible situations makes “unit testing” the program as a whole very tidious.&lt;/p&gt;

&lt;p&gt;One possible way to go is to make the code open source. Everyone can look, test, and contribute to the code, making it more reliable and easier for third-parties to audit, and for engineers to try to break (breaking things is good, because it shows a fix is needed). Moreover, this may push the industry to adopt a sngle core system instead of each developping its own. This would improve both the time to market and the reliability.&lt;/p&gt;

&lt;p&gt;Seeing the trend for Silicon Valley players to open-source their precious algorithms, especially for deep learning and artificial intelligence (&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; with Google, &lt;a href=&quot;http://arstechnica.com/information-technology/2015/12/facebooks-open-sourcing-of-ai-hardware-is-the-start-of-the-deep-learning-revolution/&quot;&gt;Torch and Big Sur&lt;/a&gt; for Facebook…), I can imagine this happening for self-driving cars. There already exists an open source Robot Operating System (ROS) on which self-driving algorithms could run, and the opportunity for an open-sourced robotic environment is big.&lt;/p&gt;

&lt;p&gt;Will the big players want to go with the open-source movement? I hope so. This is an opportunity to gain trust from governments, the public, and create a community of supporters.&lt;/p&gt;

&lt;p&gt;Why would they want to give up such amount of intellectual property developed over years of research?&lt;/p&gt;

&lt;p&gt;I think the answer is: because they have the data.&lt;/p&gt;

&lt;p&gt;The software does not run by itself. It is supported by huge amounts of data about the environment: the maps that run the car, the training images that the models use to recognize a pedestrian from a tree, or a stroller from a trash bin… Those who can extract meaningful knowledge from the world and feed it to the self-driving cars will probably be ahead of the competition. And why not give the software for free when the map that the software runs on needs a subscription?&lt;/p&gt;

&lt;p&gt;The future might prove me wrong, but as a believer in open source and self-driving cars, I can see the big potential of those two worlds colliding.&lt;/p&gt;

</description>
        <pubDate>Mon, 22 Feb 2016 02:00:05 -0800</pubDate>
        <link>http://benjamintd.github.io/blog/Open-source-software-and-sdc/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/Open-source-software-and-sdc/</guid>
        
        
        <category>sdc</category>
        
        <category>open-source</category>
        
      </item>
    
      <item>
        <title>There is no Middle Ground for Autonomous Cars</title>
        <description>&lt;p&gt;On many articles that I read about self-driving cars, there is this timeline that shows the different steps towards a fully autonomous vehicle. It first starts as a highway cruise mode, such as Tesla’s Autopilot, then progresses to more and more autonomy, with the user having to intervene every 10 miles, 100 miles, 1000 miles until we reach a Google-like future with no wheel and pedals at all. This timeline always looks progressive. There is now a commonly accepted grade of autonomy, on a scale from 1 to 4. I think the road to self driving cars will not be that smooth and continuous.&lt;/p&gt;

&lt;p&gt;I believe the reason is not that the product will not be ready or unsafe, but rather that the customers are not ready (nor safe).&lt;/p&gt;

&lt;h1 id=&quot;trusting-technology-is-easy&quot;&gt;Trusting technology is easy&lt;/h1&gt;
&lt;p&gt;Take a look at what is happening right now. Even in a setup where customers are aware that the autopilot is only an assistance system, and are liable for any damage that it might cause, drivers start looking away. We have seen &lt;a href=&quot;https://youtu.be/-okFVuHlxII?t=50&quot;&gt;videos&lt;/a&gt; of Tesla users leaving the front seat or engage in dangerous behaviors, to the point that the company may release an &lt;a href=&quot;http://mashable.com/2015/12/11/tesla-restrict-autopilot-report/&quot;&gt;upgrade&lt;/a&gt; that limits the software’s capabilities on certain roads. The lesson here is that trusting technology is incredibly easy once we see it working. The users are not to blame if they toggle the autopilot to focus on something else than boring highway driving. What is the point of having an autopilot if you still have to concentrate on the road as much as if you were driving yourself? The issue is that human drivers will still be required to keep an eye on the road, even though interventions from them will be less and less needed as technology improves. Those behaviors will happen even more when failure rates drop down to one in a thousand miles, leading to easily avoidable accidents.&lt;/p&gt;

&lt;p&gt;This problem of losing attention does not occur in a fully autonomous car, where users are not expected to take over the vehicle at any time. Until we build such a car, drivers will need to remain focused on the road and detect system failures, however rare they can be. Therefore, the transition from autopilot as an assistance system to a fully autonomous car cannot be smooth, from the driver’s perspective: you either have to remain master of your vehicle at all times, or are allowed to sleep while on the highway.&lt;/p&gt;

&lt;p&gt;In my opinion, users should never have to watch for system failures in autonomous mode. There is only one way this can happen: if the system never fails.&lt;/p&gt;

&lt;h1 id=&quot;achieveing-reliability-with-maps&quot;&gt;Achieveing reliability with maps&lt;/h1&gt;
&lt;p&gt;How can we be sure that the system never fails? Until we figure out how cars can handle any situation perfectly, we have to limit their scope.
Take the example of autonomous buses, that start arriving in &lt;a href=&quot;http://www.digitaltrends.com/cars/first-autonomous-buses-debut-in-spring-2016/&quot;&gt;Switzerland&lt;/a&gt;, France, and the &lt;a href=&quot;http://gizmodo.com/the-uss-first-autonomous-buses-will-drive-around-a-cali-1734989938&quot;&gt;US&lt;/a&gt; to name a few. This is a perfect example of a limited scope with control on the environment (think theme park shuttles, airport buses, etc.). The limited area makes it easier for the bus to perfectly know its surroundings, and makes testing and quality assurance easier. In the same fashion, it is possible to operate self-driving taxis for known origins and destinations, as it is planned for &lt;a href=&quot;http://www.japantimes.co.jp/news/2015/10/04/business/tech/self-driving-cars-let-tourists-ride-tokyo-2020-abe-says/#.VnSIOnUrI8o&quot;&gt;Tokyo Olympic Games&lt;/a&gt; in 2020. In all of these situations, the user is never in control of the vehicle, and failures become virtually impossible due to specific knowledge about the environment. Those vehicles reach full autonomy, but on a limited scope.&lt;/p&gt;

&lt;p&gt;I guess the point is that the required reliability for full autonomy will only come with an extended knowledge of the environment. All achievements in difficult settings have been made possible by having highly accurate &lt;a href=&quot;http://www.gizmag.com/self-driving-car-mexico/40013/&quot;&gt;prior mapping of the area&lt;/a&gt;. The usual approach is to have a 3D map of the environment, that enables high-precision localization. These maps are costly to make because every street needs to be surveyed regularly, with expensive equipment (Lidar, especially). This is ok to do for a few bus routes, but regular updates of all the network will be extremely expensive and time-consuming. Hopefully, better and cheaper localization can be achieved in the future when the costs of hardware decrease, which will allow more vehicles to contribute to the effort of 3D-mapping the world.&lt;/p&gt;

&lt;p&gt;This map is a static view of the world and is a way of making up for the inherent GPS imprecision and add infrastructure information such as lane markings and obstacles. It also helps identifying moving objects. However it is costly to produce and increadibly heavy in data.&lt;/p&gt;

&lt;p&gt;I believe there is a second, underestimated layer of mapping necessary for a self-driving car to navigate this static infrastructure. This is a behavior layer that would describes &lt;em&gt;how&lt;/em&gt; objects move in the infrastructure space. Of course, I am biased by the fact that I am building &lt;a href=&quot;http://www.exonav.com/&quot;&gt;such a map&lt;/a&gt;, but think about it: because of the large taxonomy of intersections and maneuvers, it is unrealistic to try to handle every situation through a single software that remains testable and maintainable (although guys at &lt;a href=&quot;http://nutonomy.com/&quot;&gt;NuTonomy&lt;/a&gt; are trying to tackle the complexity issue), and - most importantly - reliable enough to take safety critical decisions. However, I think we can know a lot about an intersection before being on the spot and compute know how to behave in that specific context. What are the possible maneuvers? Are the vehicles around me following a known trajectory? Where am I supposed to stop in order to make that left turn? Can I predict the intents of the other drivers?&lt;/p&gt;

&lt;p&gt;With some care, this information can be extracted simply from observing real drivers behave. We can do this from the extracting driving data traces from those vehicle, and perform some statistical analysis to cluster them into meaningful reference trajectories and maneuvers. We can infer the lane topology of the network without surveying the streets with expensive equipment (like what Here is doing with their &lt;a href=&quot;http://www.slashgear.com/inside-the-nokia-here-hd-maps-putting-google-on-notice-18325742/&quot;&gt;HD Map&lt;/a&gt;). This layer can serve as a backup or a template behavior to adapt in specific situations. Furthermore, it is self-validating and scales cheaply. Mobileye &lt;a href=&quot;https://www.youtube.com/watch?v=fA3bOJIEOvU&amp;amp;feature=youtu.be&amp;amp;t=2319&quot;&gt;recently announced&lt;/a&gt; its goal to produce a similar map at CES.&lt;/p&gt;

&lt;p&gt;This behavioral layer has the potential to bring the reliability of self-driving cars to the level where you can finally sleep during your daily commute, by giving context awareness to the vehicle in every tricky situation, and help cross the huge gap that I think there is from limited assistance systems to fully autonomous vehicles.&lt;/p&gt;

</description>
        <pubDate>Fri, 19 Feb 2016 02:00:03 -0800</pubDate>
        <link>http://benjamintd.github.io/blog/There-is-no-middle-ground-for-autonomous-cars/</link>
        <guid isPermaLink="true">http://benjamintd.github.io/blog/There-is-no-middle-ground-for-autonomous-cars/</guid>
        
        
        <category>sdc</category>
        
        <category>maps</category>
        
      </item>
    
  </channel>
</rss>
